{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mck_720SY76g"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbBu2eOsZKZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b889275-fb6c-4558-c7dd-e7746048b278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.43)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 28.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 43.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 45.6 MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=9cc7fc032deaa155a3ad01786de694253cf0e59b0e55140667eac773d1200f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 647 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.12.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.5.1.post0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting skorch\n",
            "  Downloading skorch-0.12.1-py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch) (0.8.10)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from skorch) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->skorch) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->skorch) (3.1.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install category_encoders\n",
        "!pip install skorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc7RBgPQYy9I"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------\n",
        "# IMPORT LIBRARIES\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import SkorchPruningCallback\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import skorch\n",
        "from skorch import NeuralNetClassifier\n",
        "\n",
        "import patsy\n",
        "import re\n",
        "from category_encoders import TargetEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Viim4HGY9yq"
      },
      "source": [
        "# Read in Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPq_-v7sZUEc",
        "outputId": "899b9aa9-bf6c-436a-d7ac-f76d7d0752d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive so that we can access google drive files\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "train = pd.read_csv('/drive/My Drive/Colab Notebooks/MMA 869 - Machine Learning & AI/training_set_features.csv')\n",
        "train_labels = pd.read_csv('/drive/My Drive/Colab Notebooks/MMA 869 - Machine Learning & AI/training_set_labels.csv')\n",
        "test = pd.read_csv('/drive/My Drive/Colab Notebooks/MMA 869 - Machine Learning & AI/test_set_features.csv')\n",
        "sub_format = pd.read_csv('/drive/My Drive/Colab Notebooks/MMA 869 - Machine Learning & AI/submission_format.csv')\n",
        "\n",
        "\n",
        "# create y label columns\n",
        "y_train_h1n1 = train_labels['h1n1_vaccine']\n",
        "y_train_seas = train_labels['seasonal_vaccine']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k4ur0Pyae-y"
      },
      "source": [
        "# Imputation Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJeCke_WajzC"
      },
      "outputs": [],
      "source": [
        "def negOne_noneTag_imputation(train_set,test_set):\n",
        "    # Fill missing numerics with -1\n",
        "    train_nums = train_set.select_dtypes('number')\n",
        "    test_nums = test_set.select_dtypes('number')\n",
        "\n",
        "    train_nums = train_nums.fillna(value=-1)\n",
        "    test_nums = test_nums.fillna(value=-1)\n",
        "\n",
        "    # Now simply fill with 'None' for the categorical features\n",
        "    train_cats = train_set.select_dtypes('object')\n",
        "    test_cats = test_set.select_dtypes('object')\n",
        "\n",
        "    train_cats = train_cats.fillna(value='None')\n",
        "    test_cats = test_cats.fillna(value='None')\n",
        "\n",
        "    train_df = pd.concat([train_nums,train_cats],axis=1)\n",
        "    test_df = pd.concat([test_nums,test_cats],axis=1)\n",
        "\n",
        "    return train_df,test_df\n",
        "\n",
        "\n",
        "def knn_noneTag_imputation(train_set,test_set):\n",
        "\n",
        "    numeric_cols = list(train_set.select_dtypes('number').columns.values)\n",
        "\n",
        "    # KNN works best with normalized data so we will normalize, fit to KNN imputer, and then reverse the normalization\n",
        "    scaler = StandardScaler()\n",
        "    scaled_train_nums = pd.DataFrame(scaler.fit_transform(train_set[numeric_cols]), columns = train_set[numeric_cols].columns, index=train_set.index)\n",
        "    scaled_test_nums = pd.DataFrame(scaler.transform(test_set[numeric_cols]), columns = test_set[numeric_cols].columns, index=test_set.index)\n",
        "\n",
        "    # Fill missing numeric with nearest neighbour values\n",
        "    knn_imputer = KNNImputer(n_neighbors=5).fit(scaled_train_nums)\n",
        "    train_num = pd.DataFrame(knn_imputer.transform(scaled_train_nums), columns = scaled_train_nums.columns, index=train_set.index)\n",
        "    test_num = pd.DataFrame(knn_imputer.transform(scaled_test_nums), columns = scaled_test_nums.columns, index=test_set.index)\n",
        "\n",
        "    # Now reverse scaling using KNN output\n",
        "    train_num = pd.DataFrame(scaler.inverse_transform(train_num), columns = train_num.columns, index=train_set.index)\n",
        "    test_num = pd.DataFrame(scaler.inverse_transform(test_num), columns = test_num.columns, index=test_set.index)\n",
        "\n",
        "    # Now simply fill with 'None' for the categorical features\n",
        "    train_cats = train_set.select_dtypes('object')\n",
        "    test_cats = test_set.select_dtypes('object')\n",
        "\n",
        "    train_cats = train_cats.fillna(value='None')\n",
        "    test_cats = test_cats.fillna(value='None')\n",
        "\n",
        "    train_df = pd.concat([train_num,train_cats],axis=1)\n",
        "    test_df = pd.concat([test_num,test_cats],axis=1)\n",
        "\n",
        "    return train_df,test_df\n",
        "\n",
        "\n",
        "def mean_none_imputation(train_set,test_set):\n",
        "    \n",
        "    numeric_cols = list(train_set.select_dtypes('number').columns.values)\n",
        "\n",
        "    num_imputer = SimpleImputer(strategy='mean').fit(train_set[numeric_cols])\n",
        "    train_nums = pd.DataFrame(num_imputer.transform(train_set[numeric_cols]), columns = train_set[numeric_cols].columns, index=train_set.index)\n",
        "    test_nums = pd.DataFrame(num_imputer.transform(test_set[numeric_cols]), columns = test_set[numeric_cols].columns, index=test_set.index)\n",
        "\n",
        "    # Don't round values as it seems to decrease auc\n",
        "    #train_nums = round(train_nums,0)\n",
        "    #test_nums = round(test_nums,0)\n",
        "\n",
        "    # Now simply fill with 'None' for the categorical features\n",
        "    train_cats = train_set.select_dtypes('object')\n",
        "    test_cats = test_set.select_dtypes('object')\n",
        "\n",
        "    train_cats = train_cats.fillna(value='None')\n",
        "    test_cats = test_cats.fillna(value='None')\n",
        "\n",
        "    train_df = pd.concat([train_nums,train_cats],axis=1)\n",
        "    test_df = pd.concat([test_nums,test_cats],axis=1)\n",
        "\n",
        "    return train_df,test_df\n",
        "\n",
        "def meanGroup_none_imputation(train_set,test_set):\n",
        "\n",
        "    # Collect numeric columns\n",
        "    numeric_cols = list(train_set.select_dtypes('number').columns.values)\n",
        "\n",
        "    # Identify the numeric columns with null values\n",
        "    cols_wNulls = pd.DataFrame(train_set[numeric_cols].isna().sum() > 0).reset_index()\n",
        "    cols_wNulls = cols_wNulls.rename(columns={\"index\": \"column\", 0: \"flag\"})\n",
        "    cols_wNulls = cols_wNulls[cols_wNulls['flag'] == True]\n",
        "\n",
        "    # For each numeric column with NaNs...\n",
        "    for col in cols_wNulls['column'].unique():\n",
        "\n",
        "        # Calculate group averages (NOTE: geo_region and age_group have 0 nulls in both train and test, otherwise we should fill those beforehand)\n",
        "        geo_age_means = train_set.groupby(['hhs_geo_region','age_group'])[col].mean().reset_index()\n",
        "        geo_means = train_set.groupby(['hhs_geo_region'])[col].mean().reset_index()\n",
        "        age_means = train_set.groupby(['age_group'])[col].mean().reset_index()\n",
        "\n",
        "        # Iterate through each row in the means and match it in rows that are NaN \n",
        "        for index,row in geo_age_means.iterrows():\n",
        "            \n",
        "            age_category = row['age_group']\n",
        "            geo_category = row['hhs_geo_region']\n",
        "            mean_value_to_impute = row[col]\n",
        "            \n",
        "            # Insert mean value for null values when geo_region and age_group match\n",
        "            train_set.loc[(train_set[col].isna()) & (train_set['hhs_geo_region']==geo_category) & (train_set['age_group']==age_category), col] = mean_value_to_impute\n",
        "\n",
        "            # Do the same for testing data\n",
        "            test_set.loc[(test_set[col].isna()) & (test_set['hhs_geo_region']==geo_category) & (test_set['age_group']==age_category), col] = mean_value_to_impute\n",
        "\n",
        "        # Not all values may be found in testing data so when we still have missing values, we will fill using the age-based means for the current column\n",
        "        if test_set[col].isna().sum() > 0:\n",
        "            age_only_mean = age_means[age_means['age_group'] == age_category]\n",
        "            age_mean_to_impute = age_only_mean[col].iat[0]\n",
        "            \n",
        "            test_set.loc[(test_set[col].isna()) & (test_set['age_group']==age_category), col] = age_mean_to_impute\n",
        "    \n",
        "    # Separate numerics from categories so we can easily combine them after imputing categorical nulls\n",
        "    train_nums = train_set.select_dtypes('number')\n",
        "    test_nums = test_set.select_dtypes('number')\n",
        "\n",
        "    # Now simply fill with 'None' for the categorical features\n",
        "    train_cats = train_set.select_dtypes('object')\n",
        "    test_cats = test_set.select_dtypes('object')\n",
        "\n",
        "    train_cats = train_cats.fillna(value='None')\n",
        "    test_cats = test_cats.fillna(value='None')\n",
        "\n",
        "    train_df = pd.concat([train_nums,train_cats],axis=1)\n",
        "    test_df = pd.concat([test_nums,test_cats],axis=1)\n",
        "\n",
        "    return train_df,test_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89cfkKXjazQp"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catFeat_to_group = ['age_group','race','hhs_geo_region','employment_status','income_poverty']\n",
        "feat_to_calc = ['doctor_recc_h1n1','doctor_recc_seasonal','h1n1_concern','h1n1_knowledge',\n",
        "                'health_insurance','chronic_med_condition','opinion_h1n1_vacc_effective',\n",
        "                'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc']\n",
        "\n",
        "def add_catMeans(training,testing,bygroupCols,calcCols):\n",
        "    \n",
        "    # for each category column we want to group things by...\n",
        "    for col in bygroupCols:\n",
        "        means = training.groupby(col)[[calcCols]].mean().add_suffix(col+'_Mean').reset_index(col)\n",
        "\n",
        "        training = training.merge(means, on=col, how='left')\n",
        "        testing = testing.merge(means, on=col, how='left')\n",
        "        \n",
        "    return training, testing\n",
        "\n",
        "\n",
        "def add_opinionMean(training,testing):\n",
        "\n",
        "    opinion_cols = ['opinion_h1n1_vacc_effective','opinion_h1n1_risk','opinion_h1n1_sick_from_vacc',\n",
        "                'opinion_seas_vacc_effective','opinion_seas_risk','opinion_seas_sick_from_vacc']\n",
        "    \n",
        "    # for each category column we want to group things by...\n",
        "    for col in opinion_cols:\n",
        "\n",
        "        training[col+'_ovrAvg'] = np.where(training[col]>training[col].mean(), 1 ,0)\n",
        "        testing[col+'_ovrAvg'] = np.where(testing[col]>training[col].mean(), 1 ,0)\n",
        "        \n",
        "    return training, testing\n",
        "\n",
        "\n",
        "formula = \"~ hhs_geo_region*education + age_group*education*sex + income_poverty*census_msa\"\n",
        "def create_matrix(training,testing,matrix_formula):\n",
        "\n",
        "    # Create flag before concat\n",
        "    training['trainTest'] = 'train'\n",
        "    testing['trainTest'] = 'test'\n",
        "\n",
        "    # Concat into one to ensure all categories are covered\n",
        "    input_data = pd.concat([training,testing],axis=0)\n",
        "    \n",
        "    # Create interaction matrix\n",
        "    interact_matrix = patsy.dmatrix(matrix_formula,input_data,return_type=\"dataframe\")\n",
        "\n",
        "    # Drop any columns that were used in the matrix because their dummy variables are now in the matrix\n",
        "    interact_matrix = interact_matrix.drop(['Intercept','hhs_geo_region','education','age_group','sex','income_poverty','census_msa'],axis=1)\n",
        "\n",
        "\n",
        "    # Combine matrix with original data\n",
        "    input_data = pd.concat([input_data,interact_matrix],axis=1)\n",
        "    \n",
        "\n",
        "    # Split data up and drop flag column created earlier\n",
        "    x_trainOutput = [input_data['trainTest']=='train']\n",
        "    x_trainOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "    \n",
        "    x_testOutput = input_data[input_data['trainTest']=='test']\n",
        "    x_testOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "\n",
        "    return x_trainOutput, x_testOutput"
      ],
      "metadata": {
        "id": "06HZ4WMrmuDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Encoding"
      ],
      "metadata": {
        "id": "Pcvf5N_Rh4Rs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4aBUGeKuf6w"
      },
      "outputs": [],
      "source": [
        "def ordinal_encoder(x_train,x_test):\n",
        "\n",
        "    # Create flag before concat\n",
        "    x_train['trainTest'] = 'train'\n",
        "    x_test['trainTest'] = 'test'\n",
        "\n",
        "    # Concat into one\n",
        "    input_data = pd.concat([x_train,x_test],axis=0)\n",
        "\n",
        "    # Ordinal column mappings\n",
        "    ordinal_cols = ['age_group','education','employment_status','income_poverty']\n",
        "    ordinal_dicts = [\n",
        "        {\"None\":0, \"18 - 34 Years\":1, \"35 - 44 Years\":2, \"45 - 54 Years\":3, \"55 - 64 Years\":4, \"65+ Years\":5},\n",
        "        {\"None\":0, \"< 12 Years\":1, \"12 Years\":2, \"Some College\":3, \"College Graduate\":4},\n",
        "        {\"None\":0, \"Unemployed\":1, \"Not in Labor Force\":2, \"Employed\":3},\n",
        "        {\"None\":0, \"Below Poverty\":1, \"<= $75,000, Above Poverty\":2, \"> $75,000\":3}\n",
        "        ]\n",
        "\n",
        "    # for each column map overwrite the original column with the ordinal mapping\n",
        "    for col,dictionary in zip(ordinal_cols,ordinal_dicts):\n",
        "        input_data[col] = input_data[col].map(dictionary)\n",
        "\n",
        "    # Split data up and drop flag column created earlier\n",
        "    x_trainOutput = input_data[input_data['trainTest']=='train']\n",
        "    x_trainOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "    \n",
        "    x_testOutput = input_data[input_data['trainTest']=='test']\n",
        "    x_testOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "\n",
        "    return x_trainOutput,x_testOutput\n",
        "\n",
        "\n",
        "def dummy_encoder(x_trainData, x_testData):\n",
        "\n",
        "    # Create flag before concat\n",
        "    x_trainData['trainTest'] = 'train'\n",
        "    x_testData['trainTest'] = 'test'\n",
        "\n",
        "    # Concat into one\n",
        "    input_data = pd.concat([x_trainData,x_testData],axis=0)\n",
        "    \n",
        "    # Loop through object columns and transform to dummy variable\n",
        "    collector = pd.DataFrame()\n",
        "    for col in input_data.select_dtypes('object'):\n",
        "        if col != 'trainTest':\n",
        "            col_dummies = pd.get_dummies(input_data[col], drop_first=True, prefix=col, prefix_sep='_')\n",
        "            collector = pd.concat([collector, col_dummies], axis=1)\n",
        "\n",
        "    # Combine encoded object data with numeric data\n",
        "    output_data = pd.concat([input_data.select_dtypes(['number']),collector,input_data['trainTest']],axis=1)\n",
        "\n",
        "    # Split data up and drop flag column created earlier\n",
        "    x_trainOutput = output_data[output_data['trainTest']=='train']\n",
        "    x_trainOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "    \n",
        "    x_testOutput = output_data[output_data['trainTest']=='test']\n",
        "    x_testOutput.drop(['trainTest'],axis=1,inplace=True)\n",
        "\n",
        "    return x_trainOutput, x_testOutput\n",
        "\n",
        "\n",
        "\n",
        "def target_encoder(x_trainCV, x_valCV, y_trainCV):\n",
        "\n",
        "    for col in x_trainCV.select_dtypes('object'):\n",
        "        target_encoder = TargetEncoder()\n",
        "\n",
        "        x_trainCV[col] = target_encoder.fit_transform(x_trainCV[col],y_trainCV)\n",
        "        x_valCV[col] = target_encoder.transform(x_valCV[col])\n",
        "\n",
        "    return x_trainCV, x_valCV\n",
        "\n",
        "\n",
        "def woe_encoder(x_trainCV,x_valCV,y_trainCV):\n",
        "\n",
        "    # Combine y_train and x_train first, as this will always be where\n",
        "    # we get the weight-of-evidence values from\n",
        "    xy = pd.concat([x_trainCV,y_trainCV],axis=1)\n",
        "\n",
        "    # For each categorical column...\n",
        "    for col in xy.select_dtypes('object'):\n",
        "\n",
        "        # Group by the vaccine column to get sum\n",
        "        cat_group = xy[[col,y_trainCV.name]].groupby([col,y_trainCV.name]).size().reset_index(name='count')\n",
        "\n",
        "        # Re-organize data so that the counts are in two columns and the categories are unique\n",
        "        vaccine_0 = cat_group[cat_group[y_trainCV.name] == 0]\n",
        "        vaccine_0['0'] = vaccine_0['count']\n",
        "        vaccine_0.drop([y_trainCV.name,'count'],axis=1,inplace=True)\n",
        "        \n",
        "        vaccine_1 = cat_group[cat_group[y_trainCV.name] == 1]\n",
        "        vaccine_1['1'] = vaccine_1['count']\n",
        "        vaccine_1.drop([y_trainCV.name,'count'],axis=1,inplace=True)\n",
        "        \n",
        "        cat_group = vaccine_0.merge(vaccine_1,on=col,how='left')\n",
        "\n",
        "\n",
        "        # Calculate percentage of 0s and 1s for each category\n",
        "        cat_group['% 0'] = cat_group['0'] / (cat_group['0']+cat_group['1'])\n",
        "        cat_group['% 1'] = cat_group['1'] / (cat_group['0']+cat_group['1'])\n",
        "        cat_group['woe'] = np.log((cat_group['% 1']/cat_group['% 0']))\n",
        "\n",
        "\n",
        "        # For each category in this feature...\n",
        "        for item in cat_group[col].unique():\n",
        "\n",
        "            # Map the WOE value of the given category to the training while overwriting the column\n",
        "            cat_woe_value = cat_group[cat_group[col] == item]\n",
        "            xy.loc[xy[col] == item, col] = cat_woe_value['woe'].iat[0]\n",
        "\n",
        "            # Do the same for testing data, but for those not found in the cat_group from training use the feature's WOE mean\n",
        "            if item not in x_valCV[col].unique():\n",
        "                x_valCV.loc[x_valCV[col] == item, col] = cat_group['woe'].mean()\n",
        "            else:\n",
        "                x_valCV.loc[x_valCV[col] == item, col] = cat_woe_value['woe'].iat[0]\n",
        "\n",
        "\n",
        "        xy[col] = xy[col].astype(float)\n",
        "        x_valCV[col] = x_valCV[col].astype(float)\n",
        "\n",
        "    # Drop y variable\n",
        "    x_fitOutput = xy.drop([y_trainCV.name],axis=1)\n",
        "    x_valOutput = x_valCV\n",
        "\n",
        "\n",
        "    return x_fitOutput, x_valOutput"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup(x_training, x_testing):\n",
        "\n",
        "    for col in ['respondent_id','index','level_0']:\n",
        "        if col in x_training.columns:\n",
        "            x_training = x_training.drop([col],axis=1)\n",
        "        if col in x_testing.columns:\n",
        "            x_testing = x_testing.drop([col],axis=1)\n",
        "    \n",
        "    # Lightgbm doesn't like certain characters in column names so remove them\n",
        "    for char in ['<', '>', '[' ,']' ,'+', '.', ':', ',']:\n",
        "        x_training.columns = x_training.columns.str.replace(char, '')\n",
        "        \n",
        "    for char in ['<', '>', '[' ,']' ,'+', '.', ':', ',']:\n",
        "        x_testing.columns = x_testing.columns.str.replace(char, '')\n",
        "        \n",
        "    return x_training, x_testing"
      ],
      "metadata": {
        "id": "SQjcyH74iLwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylxyjef8bX9N"
      },
      "source": [
        "# Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e7cFeoybeKw"
      },
      "outputs": [],
      "source": [
        "def pipe_baseline(training_data,testing_data,y_labels):\n",
        "    # ******************\n",
        "    # Imputation - Using -1 and none as indicators\n",
        "    # Ordinal Encoding\n",
        "    # Dummy Encoding - Using remaining categoricals\n",
        "    # ******************\n",
        "    \n",
        "    train_impute, test_impute = negOne_noneTag_imputation(training_data, testing_data)\n",
        "    train_ord, test_ord = ordinal_encoder(train_impute, test_impute)\n",
        "    train_encoded, test_encoded = dummy_encoder(train_ord, test_ord)\n",
        "    X_train, X_test = cleanup(train_encoded, test_encoded)\n",
        "    return X_train, X_test\n",
        "\n",
        "def pipe_meanGroupNone_ord_dummy(training_data,testing_data,y_labels):\n",
        "    # ******************\n",
        "    # Imputation - Using mean and none as indicator\n",
        "    # Ordinal Encoding\n",
        "    # Weight-of-Evidence Encoding - Using remaining categoricals\n",
        "    # ******************\n",
        "    \n",
        "    train_impute, test_impute = meanGroup_none_imputation(training_data, testing_data)\n",
        "    train_ord, test_ord = ordinal_encoder(train_impute, test_impute)\n",
        "    train_encoded, test_encoded = dummy_encoder(train_ord, test_ord)\n",
        "    X_train, X_test = cleanup(train_encoded, test_encoded)\n",
        "    return X_train, X_test\n",
        "\n",
        "def pipe_meanNone_targEncode(training_data,testing_data,y_labels):\n",
        "    # ******************\n",
        "    # Imputation - Using mean and none as indicator\n",
        "    # Target Encoding - Using remaining categoricals\n",
        "    # ******************\n",
        "    \n",
        "    train_impute, test_impute = meanGroup_none_imputation(training_data, testing_data)\n",
        "    train_encoded, test_encoded = target_encoder(train_impute, test_impute, y_labels)\n",
        "    X_train, X_test = cleanup(train_encoded, test_encoded)\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning with Optuna"
      ],
      "metadata": {
        "id": "VAL8AjSz3eYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_vaccine = y_train_h1n1 # <- change this to seasonal flu or h1n1 as needed"
      ],
      "metadata": {
        "id": "d_TEbgRcIoZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN DATA THROUGH PIPELINE\n",
        "X_train, X_test = pipe_baseline(train,test,target_vaccine)\n",
        "\n",
        "# SCALE DATA\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns, index=X_train.index)\n",
        "\n",
        "# SPLIT DATA\n",
        "X_fit, X_val, y_fit, y_val = train_test_split(X_train, target_vaccine, test_size=0.25, random_state=42)\n",
        "\n",
        "# TRANSFORM DATA TO NUMPY\n",
        "from skorch.callbacks import BatchScoring, EarlyStopping\n",
        "X_fit = X_fit.to_numpy().astype(np.float32)\n",
        "X_val = X_val.to_numpy().astype(np.float32)\n",
        "y_fit = y_fit.to_numpy().astype(np.int64)\n",
        "y_val = y_val.to_numpy().astype(np.int64)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, trial: optuna.Trial) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # We optimize the number of layers, hidden units in each layer and dropouts.\n",
        "        layers = []\n",
        "\n",
        "        # Suggest a number of layers\n",
        "        n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
        "\n",
        "        # Original input dimensions will be equal to the number of X columns\n",
        "        input_dim = len(X_train.columns)\n",
        "\n",
        "        # For each layer in the number of suggested layers (first layer included)\n",
        "        for i in range(n_layers):\n",
        "\n",
        "            # Suggest output nodes for each layer\n",
        "            output_dim = trial.suggest_int(\"n_units_layer_{}\".format(i), 4, 128, log=True)\n",
        "\n",
        "            # Add layer info to list\n",
        "            layers.append(nn.Linear(input_dim, output_dim))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "            # Now re-assign input_dimensions to the last layer's output dimensions\n",
        "            # this will then be used to create as the input dimensions for the final output layer\n",
        "            input_dim = output_dim\n",
        "\n",
        "        # Append final output layer with 2 (binary) output dimensions\n",
        "        layers.append(nn.Linear(input_dim, 2))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.model(x), dim=-1)\n",
        "\n",
        "# Define optuna objective function that is based on the auc score\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    net = skorch.NeuralNetClassifier(\n",
        "        ClassifierModule(trial),\n",
        "        max_epochs=100,\n",
        "        lr=0.1,\n",
        "        device=device,\n",
        "        callbacks=[SkorchPruningCallback(trial, \"valid_acc\")],\n",
        "    )\n",
        "\n",
        "    net.fit(X_fit, y_fit)\n",
        "\n",
        "    return roc_auc_score(y_val, net.predict_proba(X_val)[:,1])"
      ],
      "metadata": {
        "id": "ELqe2hA83cXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=2000)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "nrl4Xofl46tM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}